{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing with CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Installation of CUDA\n",
    "\n",
    "NVidia Graphic Card required!\n",
    "\n",
    "1. Install Microsoft VisualStudio (e.g. 2019 version), including C++ development environment\n",
    "2. Install NVidia CUDA Toolkit\n",
    "\n",
    "### Installation of Julia Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `C:\\Users\\d90394\\.julia\\environments\\v1.3\\Project.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n",
      "\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `C:\\Users\\d90394\\.julia\\environments\\v1.3\\Manifest.toml`\n",
      "\u001b[90m [no changes]\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "pkg\"add CUDAdrv CUDAnative CuArrays\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Testing\u001b[22m\u001b[39m CUDAdrv\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Status\u001b[22m\u001b[39m `C:\\Users\\d90394\\AppData\\Local\\Temp\\jl_LJP1qQ\\Manifest.toml`\n",
      " \u001b[90m [fa961155]\u001b[39m\u001b[37m CEnum v0.2.0\u001b[39m\n",
      " \u001b[90m [3895d2a7]\u001b[39m\u001b[37m CUDAapi v2.1.0\u001b[39m\n",
      " \u001b[90m [c5f51814]\u001b[39m\u001b[37m CUDAdrv v5.0.1\u001b[39m\n",
      " \u001b[90m [2a0f44e3]\u001b[39m\u001b[37m Base64  [`@stdlib/Base64`]\u001b[39m\n",
      " \u001b[90m [8ba89e20]\u001b[39m\u001b[37m Distributed  [`@stdlib/Distributed`]\u001b[39m\n",
      " \u001b[90m [b77e0a4c]\u001b[39m\u001b[37m InteractiveUtils  [`@stdlib/InteractiveUtils`]\u001b[39m\n",
      " \u001b[90m [8f399da3]\u001b[39m\u001b[37m Libdl  [`@stdlib/Libdl`]\u001b[39m\n",
      " \u001b[90m [56ddb016]\u001b[39m\u001b[37m Logging  [`@stdlib/Logging`]\u001b[39m\n",
      " \u001b[90m [d6f4376e]\u001b[39m\u001b[37m Markdown  [`@stdlib/Markdown`]\u001b[39m\n",
      " \u001b[90m [de0858da]\u001b[39m\u001b[37m Printf  [`@stdlib/Printf`]\u001b[39m\n",
      " \u001b[90m [9a3f8284]\u001b[39m\u001b[37m Random  [`@stdlib/Random`]\u001b[39m\n",
      " \u001b[90m [9e88b42a]\u001b[39m\u001b[37m Serialization  [`@stdlib/Serialization`]\u001b[39m\n",
      " \u001b[90m [6462fe0b]\u001b[39m\u001b[37m Sockets  [`@stdlib/Sockets`]\u001b[39m\n",
      " \u001b[90m [8dfed614]\u001b[39m\u001b[37m Test  [`@stdlib/Test`]\u001b[39m\n",
      " \u001b[90m [4ec0a83e]\u001b[39m\u001b[37m Unicode  [`@stdlib/Unicode`]\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Info: Testing using device GeForce MX150 (compute capability 6.1.0, 1.623 GiB available memory) on CUDA driver 10.2.0\n",
      "┌ Warning: Calling CUDAdrv.@profile only informs an external profiler to start.\n",
      "│ The user is responsible for launching Julia under a CUDA profiler like `nvprof`.\n",
      "│ \n",
      "│ For improved usability, launch Julia under the Nsight Systems profiler:\n",
      "│ $ nsys launch -t cuda,cublas,cudnn,nvtx julia\n",
      "└ @ CUDAdrv.Profile C:\\Users\\d90394\\.julia\\packages\\CUDAdrv\\mCr0O\\src\\profile.jl:42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Summary: | Pass  Total\n",
      "CUDAdrv       |  105    105\n",
      "\u001b[32m\u001b[1m   Testing\u001b[22m\u001b[39m CUDAdrv tests passed \n",
      "\u001b[32m\u001b[1m   Testing\u001b[22m\u001b[39m CUDAnative\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Status\u001b[22m\u001b[39m `C:\\Users\\d90394\\AppData\\Local\\Temp\\jl_RzgPEu\\Manifest.toml`\n",
      " \u001b[90m [621f4979]\u001b[39m\u001b[37m AbstractFFTs v0.5.0\u001b[39m\n",
      " \u001b[90m [79e6a3ab]\u001b[39m\u001b[37m Adapt v1.0.0\u001b[39m\n",
      " \u001b[90m [b99e7846]\u001b[39m\u001b[37m BinaryProvider v0.5.8\u001b[39m\n",
      " \u001b[90m [fa961155]\u001b[39m\u001b[37m CEnum v0.2.0\u001b[39m\n",
      " \u001b[90m [3895d2a7]\u001b[39m\u001b[37m CUDAapi v2.1.0\u001b[39m\n",
      " \u001b[90m [c5f51814]\u001b[39m\u001b[37m CUDAdrv v5.0.1\u001b[39m\n",
      " \u001b[90m [be33ccc6]\u001b[39m\u001b[37m CUDAnative v2.7.0\u001b[39m\n",
      " \u001b[90m [3a865a2d]\u001b[39m\u001b[37m CuArrays v1.6.0\u001b[39m\n",
      " \u001b[90m [864edb3b]\u001b[39m\u001b[37m DataStructures v0.17.7\u001b[39m\n",
      " \u001b[90m [0c68f7d7]\u001b[39m\u001b[37m GPUArrays v2.0.1\u001b[39m\n",
      " \u001b[90m [929cbde3]\u001b[39m\u001b[37m LLVM v1.3.3\u001b[39m\n",
      " \u001b[90m [1914dd2f]\u001b[39m\u001b[37m MacroTools v0.5.3\u001b[39m\n",
      " \u001b[90m [872c559c]\u001b[39m\u001b[37m NNlib v0.6.2\u001b[39m\n",
      " \u001b[90m [bac558e1]\u001b[39m\u001b[37m OrderedCollections v1.1.0\u001b[39m\n",
      " \u001b[90m [ae029012]\u001b[39m\u001b[37m Requires v0.5.2\u001b[39m\n",
      " \u001b[90m [a759f4b9]\u001b[39m\u001b[37m TimerOutputs v0.5.3\u001b[39m\n",
      " \u001b[90m [2a0f44e3]\u001b[39m\u001b[37m Base64  [`@stdlib/Base64`]\u001b[39m\n",
      " \u001b[90m [8ba89e20]\u001b[39m\u001b[37m Distributed  [`@stdlib/Distributed`]\u001b[39m\n",
      " \u001b[90m [b77e0a4c]\u001b[39m\u001b[37m InteractiveUtils  [`@stdlib/InteractiveUtils`]\u001b[39m\n",
      " \u001b[90m [8f399da3]\u001b[39m\u001b[37m Libdl  [`@stdlib/Libdl`]\u001b[39m\n",
      " \u001b[90m [37e2e46d]\u001b[39m\u001b[37m LinearAlgebra  [`@stdlib/LinearAlgebra`]\u001b[39m\n",
      " \u001b[90m [56ddb016]\u001b[39m\u001b[37m Logging  [`@stdlib/Logging`]\u001b[39m\n",
      " \u001b[90m [d6f4376e]\u001b[39m\u001b[37m Markdown  [`@stdlib/Markdown`]\u001b[39m\n",
      " \u001b[90m [de0858da]\u001b[39m\u001b[37m Printf  [`@stdlib/Printf`]\u001b[39m\n",
      " \u001b[90m [9a3f8284]\u001b[39m\u001b[37m Random  [`@stdlib/Random`]\u001b[39m\n",
      " \u001b[90m [ea8e919c]\u001b[39m\u001b[37m SHA  [`@stdlib/SHA`]\u001b[39m\n",
      " \u001b[90m [9e88b42a]\u001b[39m\u001b[37m Serialization  [`@stdlib/Serialization`]\u001b[39m\n",
      " \u001b[90m [6462fe0b]\u001b[39m\u001b[37m Sockets  [`@stdlib/Sockets`]\u001b[39m\n",
      " \u001b[90m [2f01184e]\u001b[39m\u001b[37m SparseArrays  [`@stdlib/SparseArrays`]\u001b[39m\n",
      " \u001b[90m [10745b16]\u001b[39m\u001b[37m Statistics  [`@stdlib/Statistics`]\u001b[39m\n",
      " \u001b[90m [8dfed614]\u001b[39m\u001b[37m Test  [`@stdlib/Test`]\u001b[39m\n",
      " \u001b[90m [4ec0a83e]\u001b[39m\u001b[37m Unicode  [`@stdlib/Unicode`]\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Info: Testing using device GeForce MX150 (compute capability 6.1.0, 1.599 GiB available memory) on CUDA driver 10.2.0 and toolkit 10.2.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ──────────────────────────────────────────────────────────────────────────────\n",
      "                                       Time                   Allocations      \n",
      "                               ──────────────────────   ───────────────────────\n",
      "       Tot / % measured:             247s / 5.15%           10.6GiB / 12.2%    \n",
      "\n",
      " Section               ncalls     time   %tot     avg     alloc   %tot      avg\n",
      " ──────────────────────────────────────────────────────────────────────────────\n",
      " LLVM middle-end          371    7.49s  58.7%  20.2ms    654MiB  49.5%  1.76MiB\n",
      "   IR generation          371    4.15s  32.6%  11.2ms    591MiB  44.7%  1.59MiB\n",
      "     emission             371    2.57s  20.2%  6.93ms    440MiB  33.3%  1.19MiB\n",
      "     rewrite              370    1.53s  12.0%  4.13ms    149MiB  11.2%   411KiB\n",
      "       lower throw        370    424ms  3.32%  1.14ms   44.8MiB  3.39%   124KiB\n",
      "       hide unreach...  1.50k    224ms  1.76%   149μs   17.4MiB  1.32%  11.9KiB\n",
      "         predecessors   1.50k    117ms  0.92%  78.0μs   10.6MiB  0.81%  7.24KiB\n",
      "         find           1.50k   65.4ms  0.51%  43.5μs    492KiB  0.04%     335B\n",
      "         replace        1.50k   35.9ms  0.28%  23.9μs   2.41MiB  0.18%  1.64KiB\n",
      "       hide trap          370   36.5ms  0.29%  98.7μs   3.16MiB  0.24%  8.74KiB\n",
      "     linking              370   16.0ms  0.13%  43.2μs    482KiB  0.04%  1.30KiB\n",
      "     clean-up             370   4.31ms  0.03%  11.6μs    671KiB  0.05%  1.81KiB\n",
      "   device library          32    1.86s  14.6%  58.1ms   45.9KiB  0.00%  1.43KiB\n",
      "   optimization           365    1.30s  10.2%  3.55ms   56.5MiB  4.27%   158KiB\n",
      "   runtime library         65   43.9ms  0.34%   676μs   59.3KiB  0.00%     934B\n",
      " validation               669    4.68s  36.7%  7.00ms    656MiB  49.6%  0.98MiB\n",
      " LLVM back-end            312    500ms  3.92%  1.60ms   4.15MiB  0.31%  13.6KiB\n",
      "   machine-code gen...    312    441ms  3.46%  1.41ms   1.00MiB  0.08%  3.27KiB\n",
      "   preparation            312   59.1ms  0.46%   190μs   3.14MiB  0.24%  10.3KiB\n",
      " device runtime lib...      8   73.6ms  0.58%  9.20ms   8.01MiB  0.61%  1.00MiB\n",
      " Julia front-end          372   3.36ms  0.03%  9.03μs   78.6KiB  0.01%     216B\n",
      " strip debug info          70    199μs  0.00%  2.85μs     0.00B  0.00%    0.00B\n",
      " ──────────────────────────────────────────────────────────────────────────────\n",
      "Test Summary: | Pass  Total\n",
      "CUDAnative    |  480    480\n",
      "\u001b[32m\u001b[1m   Testing\u001b[22m\u001b[39m CUDAnative tests passed \n",
      "\u001b[32m\u001b[1m   Testing\u001b[22m\u001b[39m CuArrays\n",
      "\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m    Status\u001b[22m\u001b[39m `C:\\Users\\d90394\\AppData\\Local\\Temp\\jl_6HmcKY\\Manifest.toml`\n",
      " \u001b[90m [621f4979]\u001b[39m\u001b[37m AbstractFFTs v0.5.0\u001b[39m\n",
      " \u001b[90m [79e6a3ab]\u001b[39m\u001b[37m Adapt v1.0.0\u001b[39m\n",
      " \u001b[90m [b99e7846]\u001b[39m\u001b[37m BinaryProvider v0.5.8\u001b[39m\n",
      " \u001b[90m [fa961155]\u001b[39m\u001b[37m CEnum v0.2.0\u001b[39m\n",
      " \u001b[90m [3895d2a7]\u001b[39m\u001b[37m CUDAapi v2.1.0\u001b[39m\n",
      " \u001b[90m [c5f51814]\u001b[39m\u001b[37m CUDAdrv v5.0.1\u001b[39m\n",
      " \u001b[90m [be33ccc6]\u001b[39m\u001b[37m CUDAnative v2.7.0\u001b[39m\n",
      " \u001b[90m [bbf7d656]\u001b[39m\u001b[37m CommonSubexpressions v0.2.0\u001b[39m\n",
      " \u001b[90m [3a865a2d]\u001b[39m\u001b[37m CuArrays v1.6.0\u001b[39m\n",
      " \u001b[90m [864edb3b]\u001b[39m\u001b[37m DataStructures v0.17.7\u001b[39m\n",
      " \u001b[90m [163ba53b]\u001b[39m\u001b[37m DiffResults v1.0.2\u001b[39m\n",
      " \u001b[90m [b552c78f]\u001b[39m\u001b[37m DiffRules v1.0.0\u001b[39m\n",
      " \u001b[90m [7a1cc6ca]\u001b[39m\u001b[37m FFTW v1.2.0\u001b[39m\n",
      " \u001b[90m [f5851436]\u001b[39m\u001b[37m FFTW_jll v3.3.9+3\u001b[39m\n",
      " \u001b[90m [1a297f60]\u001b[39m\u001b[37m FillArrays v0.8.2\u001b[39m\n",
      " \u001b[90m [f6369f11]\u001b[39m\u001b[37m ForwardDiff v0.10.8\u001b[39m\n",
      " \u001b[90m [0c68f7d7]\u001b[39m\u001b[37m GPUArrays v2.0.1\u001b[39m\n",
      " \u001b[90m [1d5cc7b8]\u001b[39m\u001b[37m IntelOpenMP_jll v2018.0.3+0\u001b[39m\n",
      " \u001b[90m [929cbde3]\u001b[39m\u001b[37m LLVM v1.3.3\u001b[39m\n",
      " \u001b[90m [856f044c]\u001b[39m\u001b[37m MKL_jll v2019.0.117+0\u001b[39m\n",
      " \u001b[90m [1914dd2f]\u001b[39m\u001b[37m MacroTools v0.5.3\u001b[39m\n",
      " \u001b[90m [872c559c]\u001b[39m\u001b[37m NNlib v0.6.2\u001b[39m\n",
      " \u001b[90m [77ba4419]\u001b[39m\u001b[37m NaNMath v0.3.3\u001b[39m\n",
      " \u001b[90m [efe28fd5]\u001b[39m\u001b[37m OpenSpecFun_jll v0.5.3+1\u001b[39m\n",
      " \u001b[90m [bac558e1]\u001b[39m\u001b[37m OrderedCollections v1.1.0\u001b[39m\n",
      " \u001b[90m [189a3867]\u001b[39m\u001b[37m Reexport v0.2.0\u001b[39m\n",
      " \u001b[90m [ae029012]\u001b[39m\u001b[37m Requires v0.5.2\u001b[39m\n",
      " \u001b[90m [276daf66]\u001b[39m\u001b[37m SpecialFunctions v0.9.0\u001b[39m\n",
      " \u001b[90m [90137ffa]\u001b[39m\u001b[37m StaticArrays v0.12.1\u001b[39m\n",
      " \u001b[90m [a759f4b9]\u001b[39m\u001b[37m TimerOutputs v0.5.3\u001b[39m\n",
      " \u001b[90m [2a0f44e3]\u001b[39m\u001b[37m Base64  [`@stdlib/Base64`]\u001b[39m\n",
      " \u001b[90m [ade2ca70]\u001b[39m\u001b[37m Dates  [`@stdlib/Dates`]\u001b[39m\n",
      " \u001b[90m [8ba89e20]\u001b[39m\u001b[37m Distributed  [`@stdlib/Distributed`]\u001b[39m\n",
      " \u001b[90m [b77e0a4c]\u001b[39m\u001b[37m InteractiveUtils  [`@stdlib/InteractiveUtils`]\u001b[39m\n",
      " \u001b[90m [76f85450]\u001b[39m\u001b[37m LibGit2  [`@stdlib/LibGit2`]\u001b[39m\n",
      " \u001b[90m [8f399da3]\u001b[39m\u001b[37m Libdl  [`@stdlib/Libdl`]\u001b[39m\n",
      " \u001b[90m [37e2e46d]\u001b[39m\u001b[37m LinearAlgebra  [`@stdlib/LinearAlgebra`]\u001b[39m\n",
      " \u001b[90m [56ddb016]\u001b[39m\u001b[37m Logging  [`@stdlib/Logging`]\u001b[39m\n",
      " \u001b[90m [d6f4376e]\u001b[39m\u001b[37m Markdown  [`@stdlib/Markdown`]\u001b[39m\n",
      " \u001b[90m [44cfe95a]\u001b[39m\u001b[37m Pkg  [`@stdlib/Pkg`]\u001b[39m\n",
      " \u001b[90m [de0858da]\u001b[39m\u001b[37m Printf  [`@stdlib/Printf`]\u001b[39m\n",
      " \u001b[90m [3fa0cd96]\u001b[39m\u001b[37m REPL  [`@stdlib/REPL`]\u001b[39m\n",
      " \u001b[90m [9a3f8284]\u001b[39m\u001b[37m Random  [`@stdlib/Random`]\u001b[39m\n",
      " \u001b[90m [ea8e919c]\u001b[39m\u001b[37m SHA  [`@stdlib/SHA`]\u001b[39m\n",
      " \u001b[90m [9e88b42a]\u001b[39m\u001b[37m Serialization  [`@stdlib/Serialization`]\u001b[39m\n",
      " \u001b[90m [6462fe0b]\u001b[39m\u001b[37m Sockets  [`@stdlib/Sockets`]\u001b[39m\n",
      " \u001b[90m [2f01184e]\u001b[39m\u001b[37m SparseArrays  [`@stdlib/SparseArrays`]\u001b[39m\n",
      " \u001b[90m [10745b16]\u001b[39m\u001b[37m Statistics  [`@stdlib/Statistics`]\u001b[39m\n",
      " \u001b[90m [8dfed614]\u001b[39m\u001b[37m Test  [`@stdlib/Test`]\u001b[39m\n",
      " \u001b[90m [cf7118a7]\u001b[39m\u001b[37m UUIDs  [`@stdlib/UUIDs`]\u001b[39m\n",
      " \u001b[90m [4ec0a83e]\u001b[39m\u001b[37m Unicode  [`@stdlib/Unicode`]\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ Info: Testing using device GeForce MX150 (compute capability 6.1.0, 1.599 GiB available memory) on CUDA driver 10.2.0 and toolkit 10.2.89\n",
      "┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n",
      "└ @ GPUArrays C:\\Users\\d90394\\.julia\\packages\\GPUArrays\\1wgPO\\src\\indexing.jl:16\n",
      "┌ Warning: Not testing CUDNN\n",
      "└ @ Main C:\\Users\\d90394\\.julia\\packages\\CuArrays\\rNxse\\test\\dnn.jl:6\n",
      "┌ Warning: Not testing CUTENSOR\n",
      "└ @ Main C:\\Users\\d90394\\.julia\\packages\\CuArrays\\rNxse\\test\\tensor.jl:7\n",
      "[ Info: Testing ForwardDiff integration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 34.04% (697.060 MiB/2.000 GiB)\n",
      "CuArrays GPU memory usage: 86.033 MiB\n",
      "BinnedPool usage: 86.033 MiB (9.264 MiB allocated, 76.769 MiB cached)\n",
      "BinnedPool efficiency: 7.82% (6.729 MiB requested, 86.033 MiB allocated)\n",
      " ────────────────────────────────────────────────────────\n",
      "                                           Time          \n",
      "                                   ──────────────────────\n",
      "         Tot / % measured:               391s / 0.91%    \n",
      "\n",
      " Section                   ncalls     time   %tot     avg\n",
      " ────────────────────────────────────────────────────────\n",
      " background task               18    613ms  17.2%  34.0ms\n",
      "   pooled free                334    363μs  0.01%  1.09μs\n",
      "   reclaim                     18   11.7ms  0.33%   652μs\n",
      "   scan                        18   23.8μs  0.00%  1.32μs\n",
      " pooled alloc               9.88k    2.87s  80.8%   290μs\n",
      "   1. try alloc             3.58k    783ms  22.1%   219μs\n",
      "   2. gc (incremental)          6   33.7ms  0.95%  5.62ms\n",
      "     pooled free               11   10.3μs  0.00%   936ns\n",
      "   3. reclaim unused            6    239μs  0.01%  39.8μs\n",
      "     reclaim                    6    224μs  0.01%  37.3μs\n",
      "     scan                       6   6.30μs  0.00%  1.05μs\n",
      "   4. try alloc                 6    299μs  0.01%  49.8μs\n",
      "   5. gc (full)                 6    1.42s  39.9%   236ms\n",
      "     pooled free               28   35.2μs  0.00%  1.26μs\n",
      "   6. reclaim unused            6    574μs  0.02%  95.6μs\n",
      "     reclaim                    6    555μs  0.02%  92.5μs\n",
      "     scan                       6   9.20μs  0.00%  1.53μs\n",
      "   7. try alloc                 6    397μs  0.01%  66.1μs\n",
      "   8. reclaim everything        6   15.4μs  0.00%  2.57μs\n",
      "     reclaim                    6   5.50μs  0.00%   917ns\n",
      "     scan                       6   3.10μs  0.00%   517ns\n",
      "   9. try alloc                 6    159μs  0.00%  26.5μs\n",
      " pooled free                7.57k   64.5ms  1.81%  8.52μs\n",
      " reclaim                        6   6.13ms  0.17%  1.02ms\n",
      " scan                           6   6.60μs  0.00%  1.10μs\n",
      " ────────────────────────────────────────────────────────\n",
      " ────────────────────────────────────────\n",
      "                           Time          \n",
      "                   ──────────────────────\n",
      " Tot / % measured:       391s / 0.20%    \n",
      "\n",
      " Section   ncalls     time   %tot     avg\n",
      " ────────────────────────────────────────\n",
      " alloc      3.60k    779ms  97.7%   216μs\n",
      " free         738   18.1ms  2.27%  24.5μs\n",
      " ────────────────────────────────────────\n",
      "Test Summary: | Pass  Total\n",
      "CuArrays      | 4799   4799\n",
      "\u001b[32m\u001b[1m   Testing\u001b[22m\u001b[39m CuArrays tests passed \n"
     ]
    }
   ],
   "source": [
    "# start test suites - this will take a while...\n",
    "pkg\"test CUDAdrv CUDAnative CuArrays\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuArrays - Usage and Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CuArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Number Generation and Transfer between CPU and GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuArray{Float32,2,CuArray{Float32,1,Nothing}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_d = CuArrays.randn(1000, 1000);\n",
    "b_d = CuArrays.randn(1000, 1000);\n",
    "typeof(a_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  601.600 μs (112 allocations: 3.83 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime CuArrays.@sync CuArrays.randn(1000, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate random array on GPU.\n",
    "\n",
    "*CuArrays.@sync* makes sure the CPU waits until GPU computation is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array{Float32,2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Array(a_d);\n",
    "b = Array(b_d);\n",
    "typeof(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.206 ms (2 allocations: 3.81 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime Array($a_d); # no sync required here because transfer to CPU memory implicitly waits for GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy arrays from GPU to CPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.128 ms (11 allocations: 3.82 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime CuArrays.@sync CuArray($a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy array from CPU to GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.497 ms (2 allocations: 3.81 MiB)\n"
     ]
    }
   ],
   "source": [
    "# CuArrays are Float32 as standard, opposed to Arrays, which are Float64. Using Float32 here to be fair.\n",
    "@btime randn(Float32, 1000, 1000); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random array in CPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.742 ms (113 allocations: 3.82 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime Array(CuArrays.randn(1000, 1000));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually faster here to generate the random numbers on GPU and tranfer them to CPU memory than creating them with CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert Array(a_d * b_d) ≈ a*b # CPU and GPU matrix multiplication give the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9.900 ms (2 allocations: 3.81 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime $a*$b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.295 ms (11 allocations: 448 bytes)\n"
     ]
    }
   ],
   "source": [
    "@btime CuArrays.@sync $a_d * $b_d;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU matrix multiplication is 4 times faster than on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OpenBLAS 0.3.5  USE64BITINT DYNAMIC_ARCH NO_AFFINITY Haswell MAX_THREADS=16\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearAlgebra.BLAS.openblas_get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that matrix multiplication on CPU uses BLAS library which is multi-threaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10.738 ms (32 allocations: 11.45 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime Array(CuArray($a) * CuArray($b));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transferring the input matrices from CPU to GPU memory, multiplication on GPU and transfer back to CPU memory takes actually longer than CPU multiplication in this case. \n",
    "\n",
    "It is essential to avoid data transfers between CPU and GPU memory as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.275 μs (4 allocations: 8.16 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6000875413417808"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime quantile(a[1,:], 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n",
      "└ @ GPUArrays C:\\Users\\d90394\\.julia\\packages\\GPUArrays\\1wgPO\\src\\indexing.jl:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  257.865 ms (15172 allocations: 631.38 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6000875413417808"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime quantile(a_d[1,:], 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantile estimator is not parallelized, therefore it is very slow on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Point-Wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.167 μs (70 allocations: 3.02 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime exp.($a_d) .+ $b_d;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  15.789 ms (2 allocations: 3.81 MiB)\n"
     ]
    }
   ],
   "source": [
    "@btime exp.($a) .+ $b;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
